


!pip install scikit-learn -q
!pip install wbdata -q





import logging
import warnings
# Suppresses warning errors, there are too many of them there about caches from wbdata
logging.getLogger("shelved_cache.persistent_cache").setLevel(logging.ERROR)
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.filterwarnings("ignore", message="KMeans is known to have a memory leak")
import wbdata
import pandas as pd
import datetime
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
from data_handler import Data_Handler
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer





# Initialise data indicators and relative names for the columns in the data
indicators_gidd = {
    "new_displacement": "New displacement",
    "hazard_type_name": "Hazard type"
}

indicators_hdi = {
    "gnipc": "Gross National Income Per Capita",
    "ineq_inc": "Income Inequality",
    "le": "Life Expectancy at Birth",
    "gii": "Gender Inequality Index",
}

indicators_wb = {
    "EN.POP.DNST": "population_density",
    "SP.DYN.IMRT.IN": "infant_mortality_rate",
    "SL.EMP.TOTL.SP.ZS": "employment_rate",
}
# Get GIDD data
gidd_data = Data_Handler.get_data_GIDD(
    client_id="92DWGXNPOLOM7DR6",
    indicators = indicators_gidd,
    iso3=None,
    start_year=2013,
    end_year=2023,
)
# Get HDI data
hdi_data = Data_Handler.get_data_HDI(
    filepath="HDR25_Composite_indices_complete_time_series.csv",
    indicators=indicators_hdi,
    countries=None,
    start_year=2013,
    end_year=2023
)
# Get WB data
wbdata = Data_Handler.get_data_WB(
    indicators = indicators_wb, 
    countries="all", 
    start_year=2013,
    end_year=2023
 )






# Pivot the HDI dataset so that each metric_name becomes a column
hdi_data_pivot = hdi_data.pivot_table(
    index=['iso3', 'year'],   
    columns='metric_name',       
    values='value'               
).reset_index()                  

# Rename columns for consistency
wbdata.rename(columns={'Year': 'year'}, inplace=True)
wbdata.rename(columns={'ISO3': 'iso3'}, inplace=True)

# Merge datasets on 'iso3' and 'year'
data = (gidd_data
        .merge(hdi_data_pivot, on=['iso3', 'year'], how='outer')
        .merge(wbdata, on=['iso3', 'year'], how='outer')
       )


hdi_iso3 = set(hdi_data_pivot['iso3'].unique())
gidd_iso3 = set(gidd_data['iso3'].unique())

print("ISO3 codes only in HDI:", hdi_iso3 - gidd_iso3)
print("ISO3 codes only in IDMC:", gidd_iso3 - hdi_iso3)


# Find the intersection of ISO3 codes
hdi_iso3 = set(hdi_data_pivot['iso3'].dropna().unique())
gidd_iso3 = set(gidd_data['iso3'].dropna().unique())

common_iso3 = hdi_iso3 & gidd_iso3  # intersection

# Keep only rows in `data` with ISO3 codes in both datasets and not missing
data = data[data['iso3'].isin(common_iso3) & data['iso3'].notna()]









# Exclude trivial columns
cols_to_check = [c for c in data.columns if c not in ['year', 'iso3']]

# Percentage of NaN values per year (average across all countries and columns)
nan_by_year = data.groupby('year')[cols_to_check].apply(lambda x: x.isna().mean().mean() * 100)

# Plot NaN values
plt.figure(figsize=(10,5))
plt.bar(nan_by_year.index.astype(str), nan_by_year.values)
plt.title('Average Percentage of Missing Data per Year (All Countries)')
plt.xlabel('Year')
plt.ylabel('Missing Data (%)')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()








# Get NaN values by country
nan_by_country = data.groupby('iso3')[cols_to_check].apply(lambda x: x.isna().mean().mean() * 100)
nan_by_country = nan_by_country.sort_values(ascending=False)

# Plot NaN values
plt.figure(figsize=(10,6))
plt.barh(range(len(nan_by_country)), nan_by_country.values, color='skyblue')  # use range for no labels
plt.axvline(x=30, color='red', linestyle='--', linewidth=2, label='30% threshold')  # red line at 30%
plt.xlabel('% of Missing Values')
plt.ylabel("Countries")
plt.title('Ratio of missing data per country')
plt.yticks([])  # remove y-axis labels
plt.legend()
plt.gca().invert_yaxis()  # largest on top
plt.show()






def nan_mean(x):
    return x.isna().mean().mean() * 100
        
before_excl = len(data["iso3"].unique())
# Compute missing-data percentage per country
country_nan_percent = data.groupby('iso3')[cols_to_check].apply(nan_mean)

# Keep only countries with <= 30% missing data
valid_countries = country_nan_percent[country_nan_percent <= 30].index

# Filter the main DataFrame
data = data[data['iso3'].isin(valid_countries)]
after_excl = len(data["iso3"].unique())

print(f"{before_excl - after_excl} countries removed from the data.")





# Get NaN values per column
nan_per_column = data[cols_to_check].isna().mean() * 100
nan_per_column_sorted = nan_per_column.sort_values(ascending=False)

# Plot NaN values
plt.figure(figsize=(10,5))
plt.bar(nan_per_column_sorted.index, nan_per_column_sorted.values)
plt.title(f'Percentage of Missing Data per Column')
plt.xlabel('Column')
plt.ylabel('Missing Data (%)')
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

print("\nMissing data percentage per column (sorted):")
print(nan_per_column_sorted)








numeric_cols = ["new_displacement", "employment_rate", "population_density", "infant_mortality_rate", "Gender Inequality Index", "Income Inequality", "Gross National Income Per Capita", "Life Expectancy at Birth"]
# Print min and max values of each column
min_max = data[numeric_cols].agg(["min", "max"]).transpose().reset_index()
min_max.columns = ["Column", "min", "max"]
print(min_max)

# Multiply GII by a 100
data["Gender Inequality Index"] = data["Gender Inequality Index"] * 100








# Inspect the remaining nan values
data.isna().sum()





imputer = SimpleImputer(strategy="median")

values_to_impute = ml_vars = ["employment_rate", "population_density", "infant_mortality_rate", "Gender Inequality Index", "Income Inequality"]

data[ml_vars] = imputer.fit_transform(data[ml_vars])




