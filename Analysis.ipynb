{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321d8c3b925a6952",
   "metadata": {},
   "source": [
    "1 L.ibrary Imports\n",
    "\n",
    "This cell installs all the necessary additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f02b32b30a3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn -q\n",
    "!pip install wbdata -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7aa82e279cc61",
   "metadata": {},
   "source": [
    "This cell imports all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f5ef8fca7873b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "# Suppresses warning errors, there are too many of them there about caches from wbdata\n",
    "logging.getLogger(\"shelved_cache.persistent_cache\").setLevel(logging.ERROR)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"KMeans is known to have a memory leak\")\n",
    "import wbdata\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from data_handler import Data_Handler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pycountry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4a95fad9508b9",
   "metadata": {},
   "source": [
    "# 2. Datasets\n",
    "## 2.1 Pulling the data\n",
    "Add a description of which indicators we chose for each data set and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05cae44dd0154d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise data indicators and relative names for the columns in the data\n",
    "indicators_gidd = {\n",
    "    \"new_displacement\": \"New displacement\",\n",
    "    \"hazard_type_name\": \"Hazard type\"\n",
    "}\n",
    "\n",
    "indicators_hdi = {\n",
    "    \"gnipc\": \"Gross National Income Per Capita\",\n",
    "    \"ineq_inc\": \"Income Inequality\",\n",
    "    \"le\": \"Life Expectancy at Birth\",\n",
    "    \"gii\": \"Gender Inequality Index\",\n",
    "}\n",
    "\n",
    "indicators_wb = {\n",
    "    \"EN.POP.DNST\": \"population_density\",\n",
    "    \"SP.DYN.IMRT.IN\": \"infant_mortality_rate\",\n",
    "    \"SL.EMP.TOTL.SP.ZS\": \"employment_rate\",\n",
    "}\n",
    "# Get GIDD data\n",
    "disp_data = Data_Handler.get_data_GIDD(\n",
    "    client_id=\"92DWGXNPOLOM7DR6\",\n",
    "    indicators = indicators_gidd,\n",
    "    iso3=None,\n",
    "    start_year=2013,\n",
    "    end_year=2023,\n",
    ")\n",
    "# Get HDI data\n",
    "hdi_data = Data_Handler.get_data_HDI(\n",
    "    filepath=\"HDR25_Composite_indices_complete_time_series.csv\",\n",
    "    indicators=indicators_hdi,\n",
    "    countries=None,\n",
    "    start_year=2013,\n",
    "    end_year=2023\n",
    ")\n",
    "# Get WB data\n",
    "wbdata = Data_Handler.get_data_WB(\n",
    "    indicators = indicators_wb, \n",
    "    countries=\"all\", \n",
    "    start_year=2013,\n",
    "    end_year=2023\n",
    " )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b0b4232702ecb7",
   "metadata": {},
   "source": [
    "do we want one column per  year and country or have it per event ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c76a7ad9c560003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the HDI dataset so that each metric_name becomes a column\n",
    "hdi_data_pivot = hdi_data.pivot_table(\n",
    "    index=['iso3', 'year'],   \n",
    "    columns='metric_name',       \n",
    "    values='value'               \n",
    ").reset_index()                  \n",
    "\n",
    "# Rename columns for consistency\n",
    "wbdata.rename(columns={'Year': 'year'}, inplace=True)\n",
    "wbdata.rename(columns={'ISO3': 'iso3'}, inplace=True)\n",
    "\n",
    "# Merge datasets on 'iso3' and 'year'\n",
    "se_data = (hdi_data_pivot\n",
    "        .merge(wbdata, on=['iso3', 'year'], how='outer')\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbcdefa2e20a6b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISO3 codes only in HDI: ['ZZA.VHHD (Unknown)', 'ZZB.HHD (Unknown)', 'ZZC.MHD (Unknown)', 'ZZD.LHD (Unknown)', 'ZZE.AS (Unknown)', 'ZZF.EAP (Unknown)', 'ZZG.ECA (Unknown)', 'ZZH.LAC (Unknown)', 'ZZI.SA (Unknown)', 'ZZJ.SSA (Unknown)', 'ZZK.WORLD (Unknown)']\n",
      "ISO3 codes only in WBData: [' (Unknown)', 'ABW (Aruba)', 'AFE (Unknown)', 'AFW (Unknown)', 'ARB (Unknown)', 'ASM (American Samoa)', 'BMU (Bermuda)', 'CEB (Unknown)', 'CHI (Unknown)', 'CSS (Unknown)', 'CUW (Curaçao)', 'CYM (Cayman Islands)', 'EAP (Unknown)', 'EAR (Unknown)', 'EAS (Unknown)', 'ECA (Unknown)', 'ECS (Unknown)', 'EMU (Unknown)', 'EUU (Unknown)', 'FCS (Unknown)', 'FRO (Faroe Islands)', 'GIB (Gibraltar)', 'GRL (Greenland)', 'GUM (Guam)', 'HPC (Unknown)', 'IBD (Unknown)', 'IBT (Unknown)', 'IDA (Unknown)', 'IDB (Unknown)', 'IDX (Unknown)', 'IMN (Isle of Man)', 'LAC (Unknown)', 'LCN (Unknown)', 'LDC (Unknown)', 'LMY (Unknown)', 'LTE (Unknown)', 'MAC (Macao)', 'MAF (Saint Martin (French part))', 'MEA (Unknown)', 'MIC (Unknown)', 'MNA (Unknown)', 'MNP (Northern Mariana Islands)', 'NAC (Unknown)', 'NCL (New Caledonia)', 'OED (Unknown)', 'OSS (Unknown)', 'PRE (Unknown)', 'PRI (Puerto Rico)', 'PSS (Unknown)', 'PST (Unknown)', 'PYF (French Polynesia)', 'SAS (Unknown)', 'SSA (Unknown)', 'SSF (Unknown)', 'SST (Unknown)', 'SXM (Sint Maarten (Dutch part))', 'TCA (Turks and Caicos Islands)', 'TEA (Unknown)', 'TEC (Unknown)', 'TLA (Unknown)', 'TMN (Unknown)', 'TSA (Unknown)', 'TSS (Unknown)', 'VGB (Virgin Islands, British)', 'VIR (Virgin Islands, U.S.)', 'WLD (Unknown)', 'XKX (Unknown)']\n"
     ]
    }
   ],
   "source": [
    "# Function to get country names from ISO3 codes\n",
    "def iso3_to_name(codes):\n",
    "    names = []\n",
    "    for code in codes:\n",
    "        try:\n",
    "            country = pycountry.countries.get(alpha_3=code)\n",
    "            names.append(f\"{code} ({country.name})\")\n",
    "        except:\n",
    "            names.append(f\"{code} (Unknown)\")\n",
    "    return sorted(names)\n",
    "\n",
    "# Your sets\n",
    "hdi_iso3 = set(hdi_data_pivot['iso3'].unique())\n",
    "gidd_iso3 = set(disp_data['iso3'].unique())\n",
    "wbdata_iso3 = set(wbdata['iso3'].unique())\n",
    "\n",
    "# Compute unique codes\n",
    "only_hdi = hdi_iso3 - wbdata_iso3\n",
    "only_wb = wbdata_iso3 - hdi_iso3\n",
    "\n",
    "# Print with country names\n",
    "print(\"ISO3 codes only in HDI:\", iso3_to_name(only_hdi))\n",
    "print(\"ISO3 codes only in WBData:\", iso3_to_name(only_wb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c1020f66d060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the intersection of ISO3 codes\n",
    "hdi_iso3 = set(hdi_data_pivot['iso3'].dropna().unique())\n",
    "wbdata_iso3 = set(wbdata['iso3'].unique())\n",
    "\n",
    "common_iso3 = hdi_iso3 & wbdata_iso3 # intersection\n",
    "\n",
    "# Keep only rows in `data` with ISO3 codes in both datasets and not missing\n",
    "se_data = se_data[se_data['iso3'].isin(common_iso3) & se_data['iso3'].notna()]\n",
    "disp_data = disp_data[disp_data['iso3'].isin(common_iso3) & disp_data['iso3'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e54189ddfb5c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing South Asia: []\n",
      "Missing East Asia: ['Macau', 'Chinese Taipei']\n",
      "Missing Southeast Asia: []\n"
     ]
    }
   ],
   "source": [
    "# Country to ISO3 dictionaries\n",
    "south_asia = {\n",
    "    \"Bangladesh\": \"BGD\",\n",
    "    \"Bhutan\": \"BTN\",\n",
    "    \"India\": \"IND\",\n",
    "    \"Maldives\": \"MDV\",\n",
    "    \"Nepal\": \"NPL\",\n",
    "    \"Pakistan\": \"PAK\",\n",
    "    \"Sri Lanka\": \"LKA\"\n",
    "}\n",
    "\n",
    "east_asia = {\n",
    "    \"China\": \"CHN\",\n",
    "    \"Hong Kong\": \"HKG\",\n",
    "    \"Macau\": \"MAC\",\n",
    "    \"Japan\": \"JPN\",\n",
    "    \"North Korea\": \"PRK\",\n",
    "    \"South Korea\": \"KOR\",\n",
    "    \"Chinese Taipei\": \"TWN\",\n",
    "    \"Mongolia\": \"MNG\"\n",
    "}\n",
    "\n",
    "southeast_asia = {\n",
    "    \"Brunei\": \"BRN\",\n",
    "    \"Cambodia\": \"KHM\",\n",
    "    \"Indonesia\": \"IDN\",\n",
    "    \"Laos\": \"LAO\",\n",
    "    \"Malaysia\": \"MYS\",\n",
    "    \"Myanmar\": \"MMR\",\n",
    "    \"Philippines\": \"PHL\",\n",
    "    \"Singapore\": \"SGP\",\n",
    "    \"Thailand\": \"THA\",\n",
    "    \"Timor-Leste\": \"TLS\",\n",
    "    \"Vietnam\": \"VNM\"\n",
    "}\n",
    "\n",
    "# Function to find missing countries by ISO3\n",
    "def find_missing_iso(region_dict, iso_list):\n",
    "    return [country for country, iso in region_dict.items() if iso not in iso_list]\n",
    "\n",
    "# Compare each region\n",
    "missing_south_asia = find_missing_iso(south_asia, common_iso3)\n",
    "missing_east_asia = find_missing_iso(east_asia, common_iso3)\n",
    "missing_southeast_asia = find_missing_iso(southeast_asia, common_iso3)\n",
    "\n",
    "print(\"Missing South Asia:\", missing_south_asia)\n",
    "print(\"Missing East Asia:\", missing_east_asia)\n",
    "print(\"Missing Southeast Asia:\", missing_southeast_asia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea67f55f544b2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter region dicts to only include ISO3 codes that are in common_iso3\n",
    "filtered_south_asia_iso = [iso for iso in south_asia.values() if iso in common_iso3]\n",
    "filtered_east_asia_iso = [iso for iso in east_asia.values() if iso in common_iso3]\n",
    "filtered_southeast_asia_iso = [iso for iso in southeast_asia.values() if iso in common_iso3]\n",
    "\n",
    "# Combine all filtered ISO3 codes into a set\n",
    "allowed_iso3 = set(filtered_south_asia_iso + filtered_east_asia_iso + filtered_southeast_asia_iso)\n",
    "\n",
    "# Filter your datasets\n",
    "se_data = se_data[se_data['iso3'].isin(allowed_iso3)]\n",
    "disp_data = disp_data[disp_data['iso3'].isin(allowed_iso3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a08769eed02c7c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 ['BGD (Bangladesh)', 'BRN (Brunei Darussalam)', 'BTN (Bhutan)', 'CHN (China)', 'HKG (Hong Kong)', 'IDN (Indonesia)', 'IND (India)', 'JPN (Japan)', 'KHM (Cambodia)', 'KOR (Korea, Republic of)', \"LAO (Lao People's Democratic Republic)\", 'LKA (Sri Lanka)', 'MDV (Maldives)', 'MMR (Myanmar)', 'MNG (Mongolia)', 'MYS (Malaysia)', 'NPL (Nepal)', 'PAK (Pakistan)', 'PHL (Philippines)', \"PRK (Korea, Democratic People's Republic of)\", 'SGP (Singapore)', 'THA (Thailand)', 'TLS (Timor-Leste)', 'VNM (Viet Nam)']\n"
     ]
    }
   ],
   "source": [
    "print(len(allowed_iso3), iso3_to_name(allowed_iso3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624e1cf3065db43",
   "metadata": {},
   "source": [
    "## 2.2 Data preprocessing\n",
    "In this section, we complete all necessary data preprocessing.\n",
    "\n",
    "We plotted the missing values by year in a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0354ab002abc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude trivial columns\n",
    "cols_to_check = [c for c in se_data.columns if c not in ['year', 'iso3']]\n",
    "\n",
    "# Percentage of NaN values per year (average across all countries and columns)\n",
    "nan_by_year = data.groupby('year')[cols_to_check].apply(lambda x: x.isna().mean().mean() * 100)\n",
    "\n",
    "# Plot NaN values\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(nan_by_year.index.astype(str), nan_by_year.values)\n",
    "plt.title('Average Percentage of Missing Data per Year (All Countries)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Missing Data (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c68af17e7b5f1",
   "metadata": {},
   "source": [
    "Although the dataset contains missing values, missingness is not uniform across time. Missing data are highest in the early period (2013–2016, approximately 35–45%), decrease substantially in 2017 (≈20%), and 2018 (≈10%). From there onwards it remains low around 10% except for 2021.\n",
    "\n",
    "Excluding early years would therefore disproportionately remove information from the beginning of the analysis period and compress the time dimension. Because the missingness declines  over time and is driven primarily by data availability rather than  changes in the underlying phenomena, retaining all years allows temporal continuity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c472ba141186956f",
   "metadata": {},
   "source": [
    "Next, we plot countries and the percentage of missing values per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99680df69c0c2604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NaN values by country\n",
    "nan_by_country = se_data.groupby('iso3')[cols_to_check].apply(lambda x: x.isna().mean().mean() * 100)\n",
    "nan_by_country = nan_by_country.sort_values(ascending=False)\n",
    "\n",
    "# Plot NaN values\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(range(len(nan_by_country)), nan_by_country.values, color='skyblue')  # use range for no labels\n",
    "plt.axvline(x=30, color='red', linestyle='--', linewidth=2, label='30% threshold')  # red line at 30%\n",
    "plt.xlabel('% of Missing Values')\n",
    "plt.ylabel(\"Countries\")\n",
    "plt.title('Ratio of missing data per country')\n",
    "plt.yticks([])  # remove y-axis labels\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()  # largest on top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b41e611e64a945",
   "metadata": {},
   "source": [
    "From the graph, we can see that a lot of the countries have a majority of their data missing. We have decided to exclude countries that have more than 70 per cent data missing, which is indicated by the red line in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a9005616fe140",
   "metadata": {},
   "source": [
    "Now, we want to inspect the trends of missing data between different factors in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a075561d5ec803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NaN values per column\n",
    "nan_per_column = se_data[cols_to_check].isna().mean() * 100\n",
    "nan_per_column_sorted = nan_per_column.sort_values(ascending=False)\n",
    "\n",
    "# Plot NaN values\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(nan_per_column_sorted.index, nan_per_column_sorted.values)\n",
    "plt.title(f'Percentage of Missing Data per Column')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Missing Data (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMissing data percentage per column (sorted):\")\n",
    "print(nan_per_column_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a020993221d99f",
   "metadata": {},
   "source": [
    "There is no feature with  significant amount of missing data wherefore we can keep all chosen indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31fa6edabaa5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your dataset is already loaded as se_data\n",
    "# se_data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Check which columns have missing data\n",
    "print(se_data.isnull().sum())\n",
    "\n",
    "# Interpolation (good for numerical, continuous data)\n",
    "# Method='linear' fills missing values based on neighboring data\n",
    "se_data['Gender Inequality Index'] = se_data['Gender Inequality Index'].interpolate(method='linear')\n",
    "se_data['infant_mortality_rate'] = se_data['infant_mortality_rate'].interpolate(method='linear')\n",
    "se_data['Income Inequality'] = se_data['Income Inequality'].interpolate(method='linear')\n",
    "\n",
    "# If you want to do extrapolation for missing values at the start or end, you can use 'fillna' with method='bfill' or 'ffill'\n",
    "# Forward fill\n",
    "se_data['Gender Inequality Index'] = se_data['Gender Inequality Index'].fillna(method='ffill')\n",
    "se_data['infant_mortality_rate'] = se_data['infant_mortality_rate'].fillna(method='ffill')\n",
    "se_data['Income Inequality'] = se_data['Income Inequality'].fillna(method='ffill')\n",
    "\n",
    "# Backward fill as backup\n",
    "se_data['Gender Inequality Index'] = se_data['Gender Inequality Index'].fillna(method='bfill')\n",
    "se_data['infant_mortality_rate'] = se_data['infant_mortality_rate'].fillna(method='bfill')\n",
    "se_data['Income Inequality'] = se_data['Income Inequality'].fillna(method='bfill')\n",
    "\n",
    "# Check again for missing values\n",
    "print(se_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb80b5e8b904524",
   "metadata": {},
   "source": [
    "## 2.3 Uniformize data\n",
    "Next, we look at the minimum and maximum values each column can obtain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9d3469bd83f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\"employment_rate\", \"population_density\", \"infant_mortality_rate\", \"Gender Inequality Index\", \"Income Inequality\", \"Gross National Income Per Capita\", \"Life Expectancy at Birth\"]\n",
    "# Print min and max values of each column\n",
    "min_max = se_data[numeric_cols].agg([\"min\", \"max\"]).transpose().reset_index()\n",
    "min_max.columns = [\"Column\", \"min\", \"max\"]\n",
    "print(min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8749d2aa347bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your dataset is se_data\n",
    "# Columns: employment_rate, population_density, infant_mortality_rate, Gender Inequality Index, Income Inequality, Gross National Income Per Capita, Life Expectancy at Birth\n",
    "\n",
    "columns = ['employment_rate', 'population_density', 'infant_mortality_rate', \n",
    "           'Gender Inequality Index', 'Income Inequality', \n",
    "           'Gross National Income Per Capita', 'Life Expectancy at Birth']\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.histplot(se_data[col], bins=30, kde=True, color='skyblue')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd2d46f0d894fe",
   "metadata": {},
   "source": [
    "What do we do with values?\n",
    "\n",
    "new displacement --> number of people | depends, could improve in the future if we apply log transform (some models can deal without it, but linear models probably not)\n",
    "\n",
    "emplyment rate --> 0-100% is fine\n",
    "\n",
    "population density --> not sure | log transform?\n",
    "\n",
    "infant mortality --> not sure | log transform\n",
    "\n",
    "gii --> 0-1 so make 0-100 then fine\n",
    "\n",
    "income --> 0-100 is fine\n",
    "\n",
    "gni --> not sure | log transform\n",
    "\n",
    "life expectancy --> not sure \n",
    "\n",
    "normalize all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a8f65f76fc0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_data.to_csv(\"socio_economic_data.csv\", index=False)\n",
    "disp_data.to_csv(\"displacement_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700e0984b9fa4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_data = pd.read_csv(\"displacement_data.csv\")\n",
    "se_data = pd.read_csv(\"socio_economic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1913f68c69075761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
