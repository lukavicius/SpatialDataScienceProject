{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499464943fa95ba0",
   "metadata": {},
   "source": [
    "1.2 Library Imports\n",
    "This cell installs all the necessary additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373b817e2b05f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn -q\n",
    "!pip install wbdata -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d7d4858faf968",
   "metadata": {},
   "source": [
    "This cell imports all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c758d448276103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "# Suppresses warning errors, there are too many of them there about caches from wbdata\n",
    "logging.getLogger(\"shelved_cache.persistent_cache\").setLevel(logging.ERROR)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"KMeans is known to have a memory leak\")\n",
    "import wbdata\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from data_handler import Data_Handler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7971758593180422",
   "metadata": {},
   "source": [
    "# 2. Datasets\n",
    "## 2.1 Pulling the data\n",
    "Describe which indicators we chose for each data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790d3c31122f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise data indicators and relative names for the columns in the data\n",
    "indicators_gidd = {\n",
    "    \"new_displacement\": \"New displacement\",\n",
    "    \"hazard_type_name\": \"Hazard type\"\n",
    "}\n",
    "\n",
    "indicators_hdi = {\n",
    "    \"gnipc\": \"Gross National Income Per Capita\",\n",
    "    \"ineq_inc\": \"Income Inequality\",\n",
    "    \"le\": \"Life Expectancy at Birth\",\n",
    "    \"gii\": \"Gender Inequality Index\",\n",
    "}\n",
    "\n",
    "indicators_wb = {\n",
    "    \"EN.POP.DNST\": \"population_density\",\n",
    "    \"SP.DYN.IMRT.IN\": \"infant_mortality_rate\",\n",
    "    \"SL.EMP.TOTL.SP.ZS\": \"employment_rate\",\n",
    "}\n",
    "# Get WB data\n",
    "wbdata = Data_Handler.get_data_WB(\n",
    "    indicators = indicators_wb,\n",
    "    countries=\"all\",\n",
    "    start_year=1990,\n",
    "    end_year=2024\n",
    " )\n",
    "# Get GIDD data\n",
    "gidd_data = Data_Handler.get_data_GIDD(\n",
    "    client_id=\"92DWGXNPOLOM7DR6\",\n",
    "    indicators = indicators_gidd,\n",
    "    iso3=None,\n",
    "    start_year=2013,\n",
    "    end_year=2023,\n",
    ")\n",
    "# Get HDI data\n",
    "hdi_data = Data_Handler.get_data_HDI(\n",
    "    filepath=\"HDR25_Composite_indices_complete_time_series.csv\",\n",
    "    indicators=indicators_hdi,\n",
    "    countries=None,\n",
    "    start_year=2013,\n",
    "    end_year=2023\n",
    ")\n",
    "# Get WB data\n",
    "wbdata = Data_Handler.get_data_WB(\n",
    "    indicators = indicators_wb, \n",
    "    countries=\"all\", \n",
    "    start_year=2013,\n",
    "    end_year=2023\n",
    " )\n",
    "wbdata.rename(columns={'ISO3': 'iso3'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede450f6d1ad6449",
   "metadata": {},
   "source": [
    "do we want one column per  year and country or have it per event ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977570018746c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the HDI dataset so that each metric_name becomes a column\n",
    "hdi_data_pivot = hdi_data.pivot_table(\n",
    "    index=['iso3', 'year'],   \n",
    "    columns='metric_name',       \n",
    "    values='value'               \n",
    ").reset_index()                  \n",
    "\n",
    "# Rename columns for consistency\n",
    "wbdata.rename(columns={'Year': 'year'}, inplace=True)\n",
    "\n",
    "# Merge datasets on 'iso3' and 'year'\n",
    "data = (gidd_data\n",
    "        .merge(hdi_data_pivot, on=['iso3', 'year'], how='outer')\n",
    "        .merge(wbdata, on=['iso3', 'year'], how='outer')\n",
    "       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3631e3e82505a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdi_iso3 = set(hdi_data_pivot['iso3'].unique())\n",
    "gidd_iso3 = set(gidd_data['iso3'].unique())\n",
    "\n",
    "print(\"ISO3 codes only in HDI:\", hdi_iso3 - gidd_iso3)\n",
    "print(\"ISO3 codes only in IDMC:\", gidd_iso3 - hdi_iso3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12b8c077231f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the intersection of ISO3 codes\n",
    "hdi_iso3 = set(hdi_data_pivot['iso3'].dropna().unique())\n",
    "gidd_iso3 = set(gidd_data['iso3'].dropna().unique())\n",
    "\n",
    "common_iso3 = hdi_iso3 & gidd_iso3  # intersection\n",
    "\n",
    "# Keep only rows in `data` with ISO3 codes in both datasets and not missing\n",
    "data = data[data['iso3'].isin(common_iso3) & data['iso3'].notna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5d16114bc24c8",
   "metadata": {},
   "source": [
    "based on this do we get rid of countries??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a244264e21c9531",
   "metadata": {},
   "source": [
    "## 2.2 Data preprocessing\n",
    "In this section, we complete all necessary data preprocessing.\n",
    "\n",
    "We plotted the missing values by year in a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d1efb998876cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude trivial columns\n",
    "cols_to_check = [c for c in data.columns if c not in ['year', 'iso3']]\n",
    "\n",
    "# Percentage of NaN values per year (average across all countries and columns)\n",
    "nan_by_year = data.groupby('year')[cols_to_check].apply(lambda x: x.isna().mean().mean() * 100)\n",
    "\n",
    "# Plot NaN values\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(nan_by_year.index.astype(str), nan_by_year.values)\n",
    "plt.title('Average Percentage of Missing Data per Year (All Countries)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Missing Data (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae02fc556c28af",
   "metadata": {},
   "source": [
    "Eliminate  specfic years if needed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4b5fff5ec2004",
   "metadata": {},
   "source": [
    "Next, we plot countries and the percentage of missing values per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9221135b7a7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NaN values by country\n",
    "nan_by_country = data.groupby('iso3')[cols_to_check].apply(lambda x: x.isna().mean().mean() * 100)\n",
    "nan_by_country = nan_by_country.sort_values(ascending=False)\n",
    "\n",
    "# Plot NaN values\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(range(len(nan_by_country)), nan_by_country.values, color='skyblue')  # use range for no labels\n",
    "plt.axvline(x=30, color='red', linestyle='--', linewidth=2, label='30% threshold')  # red line at 30%\n",
    "plt.xlabel('% of Missing Values')\n",
    "plt.ylabel(\"Countries\")\n",
    "plt.title('Ratio of missing data per country')\n",
    "plt.yticks([])  # remove y-axis labels\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()  # largest on top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ad7e50adf3fd8",
   "metadata": {},
   "source": [
    "From the graph, we can see that a lot of the countries have a majority of their data missing. We have decided to exclude countries that have more than 70 per cent data missing, which is indicated by the red line in the graph.\n",
    "Eliminate countries if needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a38222e3d1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, we want to inspect the trends of missing data between different factors in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634abb5aa1ff1bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NaN values per column\n",
    "nan_per_column = data[cols_to_check].isna().mean() * 100\n",
    "nan_per_column_sorted = nan_per_column.sort_values(ascending=False)\n",
    "\n",
    "# Plot NaN values\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(nan_per_column_sorted.index, nan_per_column_sorted.values)\n",
    "plt.title(f'Percentage of Missing Data per Column')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Missing Data (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMissing data percentage per column (sorted):\")\n",
    "print(nan_per_column_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96c7ac9427471ea",
   "metadata": {},
   "source": [
    "Eliminate factor if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f782ddcf341163bb",
   "metadata": {},
   "source": [
    "## 2.3 Uniformize data\n",
    "Next, we look at the minimum and maximum values each column can obtain. All factors except for GII are measured from 0 to 100. We have decided to multiply the GII by a factor of 100, as this will later help us visualise the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997afdf28d9cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print min and max values of each column\n",
    "min_max = data.agg(['min', 'max']).transpose().reset_index()\n",
    "min_max.columns = ['Column', 'Min', 'Max']\n",
    "print(min_max)\n",
    "\n",
    "# Multiply GII by a 100\n",
    "data['GII'] = data['GII'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bb36fc409d0b0",
   "metadata": {},
   "source": [
    "## 2.4 Missing values\n",
    "We examine the number of missing values in the dataset to determine an appropriate strategy for handling them. Addressing missing data is essential because it can bias results, reduce the reliability of our analyses, and hinder modeling performance. Many machine learning algorithms cannot process missing values directly and require complete input data. Therefore, understanding the extent and pattern of missingness ensures that subsequent analyses and models are both accurate and valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ee4bb5040844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the remaining nan values\n",
    "data.isna().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
